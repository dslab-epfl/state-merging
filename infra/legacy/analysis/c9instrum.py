'''
Instrumentation data manipulation

Created on Jan 24, 2010

@author: stefan
'''

import sys
import subprocess
import os.path

from zipfile import ZipFile
import re
import binascii
import pickle
import random
import datetime

KLEE_STATS_FILE = "run.stats"
DEFAULT_STATS_FILE = "c9-stats.txt"
DEFAULT_EVENTS_FILE = "c9-events.txt"

COVERAGE_EXTENSION = ".cov"
LLVM_EXTENSION = ".bc"

TEMPORARY_FILE=True

################################################################################
# General Tools
################################################################################

def compute_worker_count(expDirName):
  m = re.match(".*-(.*)w$", expDirName)
  if m is None:
    assert expDirName.endswith("-klee")
    return 0

  return int(m.group(1))

################################################################################
# Statistics and Events Data Structures
################################################################################

class Statistics:
  #
  # Cloud9-output data
  #
  TOTAL_PROC_INSTRUCTIONS = 0

  TOTAL_PROC_JOBS = 1
  TOTAL_REPLAYED_JOBS = 14
  TOTAL_EXPORTED_JOBS = 8
  TOTAL_IMPORTED_JOBS = 9
  TOTAL_DROPPED_JOBS = 10
  
  TOTAL_FORKED_STATES = 15
  TOTAL_FINISHED_STATES = 16
  
  TOTAL_TREE_PATHS = 17
  
  TOTAL_REPLAY_INSTRUCTIONS = 20
  
  CURRENT_JOB_COUNT = 11
  CURRENT_ACTIVE_STATE_COUNT = 18
  CURRENT_STATE_COUNT = 19
  
  #
  # Testcase generated data
  #
  IS_TESTCASE = 100   

  IS_BUG      = 101
  IS_CRASH    = 102
  IS_ABNORMAL = 103
  IS_NORMAL   = 104
  CURRENT_TEST_FILE    = 105
  CURRENT_TEST_COMMAND = 106
  CURRENT_USER_OUTPUT  = 107    
  CURRENT_SUDO_OUTPUT  = 108
  CURRENT_KTEST_ERROR  = 109

  #
  # Common data contained generated by the coverage engine
  #
  CURRENT_REPLAY_TIME  = 200
  CURRENT_RUNNING_TIME = 201
  CURRENT_COVERAGE     = 202

  #
  # Source information
  #
  SOURCE_KLEE = 0
  SOURCE_CLOUD9 = 1

class Events:
  TEST_CASE = 0
  ERROR_CASE = 1
  JOB_EXECUTION_STATE = 2
  TIMEOUT = 3
  
  INSTRUCTION_BATCH = 4
  REPLAY_BATCH = 5
  SMT_SOLVE = 6
  SAT_SOLVE = 7
  CONSTRAINT_SOLVE = 8


class StatisticEntry:
  def __init__(self):
    self.timeStamp = None
    self.source = None
    self.stats = { }

class EventEntry:
  def __init__(self):
    self.timeStamp = None
    self.id = None
    self.value = None
    

def _parse_stamp(stamp_str):
  paranthesis_expr = re.match("\[(?P<time>[0-9]*.[0-9]*)\]", stamp_str)
  
  if paranthesis_expr is not None:
    return float(paranthesis_expr.group("time"))
  else:
    return float(stamp_str)
  
def parse_timer(timer_str):
  timings = []
  
  tokens = timer_str.split()
  
  for i in range(0, len(tokens) // 2):
    timings.append(float(tokens[2*i+1]))

  appendix = tokens[-1] if len(tokens) % 2 == 1 else None
  
  return timings, appendix


def _parse_stats(f, file_name):
  result = []
  
  for line in f:
    line=line.replace("\n","")
    line_words = line.split()
    try:
      entry = StatisticEntry()
      entry.timeStamp = _parse_stamp(line_words[0])
      entry.source = Statistics.SOURCE_CLOUD9
      
      for stat in line_words[1:]:
        stat_pair = stat.split('=')
        entry.stats[int(stat_pair[0])] = int(stat_pair[1])
        
        result.append(entry)
    except:
      sys.stderr.write("Invalid Cloud9 statistics row in file %s: %s\n" % (file_name, line))
    
    return result



def _parse_klee_stats(f, file_name):
  result = []
  skipped_first_line = False
  
  for line in f:
    # skip the first line (the caption)
    if not skipped_first_line:
      skipped_first_line=True
      continue
    
    line = line.replace("(","").replace(")","").replace("\n","")
    line_words = line.split(",")
    
    try:
      entry = StatisticEntry()
      entry.timeStamp = _parse_stamp(line_words[10])
      entry.source = Statistics.SOURCE_KLEE
      # the only information we can gather
      entry.stats[Statistics.TOTAL_PROC_INSTRUCTIONS] = int(line_words[0])
      #entry.stats[Statistics.CURRENT_PATH_COUNT] = int(line_words[5])
            
      result.append(entry)
    except:
      sys.stderr.write("Invalid KLEE statistics row in file %s: %s\n" % (file_name, line))
    
    return result



def _parse_events(f, file_name):
    result = []

    for line in f:
        line = line.replace("\n","")
        line_words = line.split()
        try:
            entry = EventEntry()
            entry.timeStamp = _parse_stamp(line_words[0])
            entry.id = int(line_words[1])
    
            entry.value = " ".join(line_words[2:])
            
            result.append(entry)
        except:
            sys.stderr.write("Invalid events row in file %s: %s\n" % (file_name, line))
    
    return result

def parse_stats(file_name):
    is_klee = False
    if file_name.endswith(KLEE_STATS_FILE):
        is_klee = True
            
    try:
        f = open(file_name, "r")
        if is_klee:
            result = _parse_klee_stats(f, file_name)
        else:
            result = _parse_stats(f, file_name)
        f.close()
    except IOError:
        sys.stderr.write("IOError when parsing events - most likely due to empty archive from crashed worker.\n")
        raise
    
    return result

def parse_events(file_name):
    try:
        f = open(file_name, "r")
        result = _parse_events(f, file_name)
        f.close()
    except IOError:
        sys.stderr.write("IOError when parsing events - most likely due to empty archive from crashed worker.\n")
        return None
    
    return result
  
################################################################################
# Load Balancing Data Structures
################################################################################

class TransferEvent:
  def __init__(self):
    self.timeStamp = None
    self.srcWorker = None
    self.dstWorker = None
    self.amount = None
    
class WorkerUpdate:
  def __init__(self):
    self.timeStamp = None
    self.worker = None # 0 means a total, otherwise it's a worker ID
    self.amount = None
    
_timestamp_re = re.compile(r"\[(.*?)\]")

_xfer_decision_re = re.compile(r"request from (.*) to (.*) for (.*) states")
_stat_update_re = re.compile(r"\t\[(.*)\] (?:(?:for worker (.*))|(?:IN TOTAL))")
    
def _parse_xferevent(logLine):
  lbMatch = _xfer_decision_re.search(logLine)
  
  if lbMatch is None:
    return None
  
  stampMatch = _timestamp_re.match(logLine)
  assert stampMatch is not None
  
  result = TransferEvent()
  result.timeStamp = float(stampMatch.group(1))
  
  result.srcWorker = int(lbMatch.group(1))
  result.dstWorker = int(lbMatch.group(2))
  result.amount = int(lbMatch.group(3))
  
  return result
  
def _parse_stat_update(logLine):
  statMatch = _stat_update_re.search(logLine)
  
  if statMatch is None:
    return None
  
  stampMatch = _timestamp_re.match(logLine)
  assert stampMatch is not None
  
  result = WorkerUpdate()
  result.timeStamp = float(stampMatch.group(1))
  
  if statMatch.group(2) is None:
    result.worker = 0
  else:
    result.worker = int(statMatch.group(2))
    
  result.amount = int(statMatch.group(1))
  
  return result

def parse_lb_log(f):
  events = []
  for line in f:
    result = _parse_stat_update(line)
    
    if result is not None:
      events.append(result)
      continue
    
    result = _parse_xferevent(line)
    
    if result is not None:
      events.append(result)
      continue
    
  return events

################################################################################
# Coverage Data Structures
################################################################################

class CoverageBit:
  def __init__(self, value = None, total = None, perc = None):
    self.value = value
    self.total = total
    self.perc = perc
        
class CoverageState:
  def __init__(self):
    self.timeStamp = None
    self.globCov = None
    self.localCov = None
    self.compCov = { }
    
_DATUM_FORMAT = r"(.*)=([^/]*)(/(.*)\((.*)\))?$"
_datum_re = re.compile(_DATUM_FORMAT)

def _parse_datum(datum):
  m = _datum_re.match(datum)
  
  name = m.group(1)
  
  covbit = CoverageBit()
  covbit.value = int(m.group(2))
  
  if m.group(3) is None:
    return (name, covbit)
  
  covbit.total = int(m.group(4))
  covbit.perc = float(m.group(5))
    
  return (name, covbit)

def _parse_coverage_state(data, perFunction):
  dataTokens = data.split()
  
  assert len(dataTokens) > 1
  
  res = CoverageState()
  
  res.timeStamp = float(dataTokens[0])
  
  for datum in dataTokens[1:]:
    (name, covbit) = _parse_datum(datum)
    
    #print "name: %s value: %d total: %d perc: %f" % (name, covbit.value, covbit.total, covbit.perc)
    
    if name == "<global>":
      res.globCov = covbit
    elif name == "<local>":
      res.localCov = covbit
    else:
      if not perFunction:
        break
      res.compCov[name] = covbit
      
  return res  

def parse_coverage_file(f, perFunction=True):
  covStates = []
  
  for line in f:
    state = _parse_coverage_state(line, perFunction)
    covStates.append(state)
    
  return covStates

def parse_coverage_data(dirName, workerCount, perFunction=True):
  covData = []
  for i in range(1, workerCount+1):
    with open(dirName + ("/worker%d" % i) + "/c9-coverage.txt") as f:
      covStates = parse_coverage_file(f, perFunction)
      covData.append(covStates)
  
  return covData

def light_parse_coverage_data(dirName, workerCount, perFunction=True):
  # First, choose the most representative worker
  maxTimeStamp = 0.0
  reprWorker = 0
  for i in range(1, workerCount+1):
    with open(dirName + ("/worker%d" % i) + "/c9-coverage.txt") as f:
      for line in f:
        pos = line.find(" ")
        timeStamp = float(line[0:pos])
        
      if timeStamp > maxTimeStamp:
        maxTimeStamp = timeStamp
        reprWorker = i
  
  print "%d selected as the leader" % reprWorker
  covData = []
  
  # Inserting the coverage of the leader
  with open(dirName + ("/worker%d" % reprWorker) + "/c9-coverage.txt") as f:
    covStates = parse_coverage_file(f, perFunction)
    covData.append(covStates)
        
  return covData

        
################################################################################
# Experimental Raw Archive Manipulation
################################################################################


def parse_results(results, stats_file=DEFAULT_STATS_FILE, events_file=DEFAULT_EVENTS_FILE):
    events = []
    stats = []
    for dir_name in results:
        new_events = parse_events(dir_name + "/" + events_file)
        new_stats = parse_stats(dir_name + "/" + stats_file)
        if new_events != None:
            events += new_events
        if new_stats != None:
            stats += new_stats
    
    return { "stats": stats, "events": events }
